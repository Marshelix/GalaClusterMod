# -*- coding: utf-8 -*-
"""
Created on Sun May 24 10:07:00 2020

@author: modified from https://www.katnoria.com/mdn/ , a tutorial on tf2 gdns

Made to fit the Einasto profile data generated by Martin Sanner

"""

import numpy as np
import matplotlib.pyplot as plt
import EinastoSim
import h5py




import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
import tensorflow as tf
from copy import deepcopy

def calc_pdf(y, mu, var):
    """Calculate component density"""
    value = tf.subtract(y, mu)**2
    value = (1/tf.math.sqrt(2 * np.pi * var)) * tf.math.exp((-1/(2*var)) * value)
    return value

def pdf_np(y, mu, var):
    n = np.exp((-(y-mu)**2)/(2*var))
    d = np.sqrt(2 * np.pi * var)
    return n/d

def mdn_loss(y_true, pi, mu, var):
    """MDN Loss Function
    The eager mode in tensorflow 2.0 makes is extremely easy to write 
    functions like these. It feels a lot more pythonic to me.
    """
    #throw away first few y values
    out = calc_pdf(y_true, mu, var)
    # multiply with each pi and sum it
    out = tf.multiply(out, pi)
    out = tf.reduce_sum(out, 1, keepdims=True)
    out = -tf.math.log(out + 1e-10)
   # print(tf.reduce_mean(out))
    return tf.reduce_mean(out)


@tf.function
def train_step(model, optimizer, train_x, train_y):
    # GradientTape: Trace operations to compute gradients
    with tf.GradientTape() as tape:
        pi_, mu_, var_ = model(train_x, training=True)
        # calculate loss
        loss = mdn_loss(train_y, pi_, mu_, var_)
    # compute and apply gradients
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss

def sample_predictions(pi_vals, mu_vals, var_vals, samples=10):
    n, k = pi_vals.shape
    l_out = 1
    # print('shape: ', n, k, l)
    # place holder to store the y value for each sample of each row
    out = np.zeros((n, samples, l_out))
    for i in range(n):
        for j in range(samples):
            # for each sample, use pi/probs to sample the index
            # that will be used to pick up the mu and var values
            idx = np.random.choice(range(k), p=pi_vals[i])
            for li in range(l_out):
                for kdist in range(k):
                    out[i,j,li] += pi_vals[i][kdist]*np.random.normal(mu_vals[i,kdist*(li+l_out)],np.sqrt(var_vals[i,kdist]))
                # Draw random sample from gaussian distribution
                #out[i,j,li] = np.random.normal(mu_vals[i, idx*(li+l_out)], np.sqrt(var_vals[i, idx]))
    return out    

#fixed sigma activation
# taken from https://github.com/cpmpercussion/keras-mdn-layer/blob/master/mdn/__init__.py
def elu_plus(x):
    return tf.keras.activations.elu(x)+1

if __name__ == "__main__":
    sample_profiles,profile_params,associated_r = EinastoSim.generate_n_random_einasto_profile_maggie(1000)
    sample_profiles_logged = np.asarray([np.log(p) for p in sample_profiles]).astype(np.float64)
    #EinastoSim.print_params(profile_params[0])
    
    def create_input_vectors(profile_params, assoc_r):
        assert len(assoc_r) == len(profile_params), "mismatch between parameter and r lengths"
        N = len(assoc_r)
        print("Generating {} elements from {} pairs".format(len(assoc_r[0])*N,N))
        input_vecs = []
        for i in range(N):
            base_vec = profile_params[i].copy()
            current_vec = []
            for j in range(len(assoc_r[i])):
                current_vec = base_vec.copy()
                r = assoc_r[i][j]
                current_vec.append(r)
                assert len(current_vec) == (len(base_vec) +1), "Appended more than one element"
                input_vecs.append(current_vec)
        return input_vecs
    
    X_full = create_input_vectors(profile_params, associated_r) 
    X_full = np.asarray(X_full).astype(np.float64)
    l = 1+len(profile_params[0])    #current r and all params
    # Number of gaussians to represent the multimodal distribution
    k = 4
    # Network
    input = tf.keras.Input(shape=(l,))
    input_transfer_layer = tf.keras.layers.Dense(1,activation = None,dtype = tf.float64)
    layer = tf.keras.layers.Dense(50, activation='tanh', name='baselayer',dtype = tf.float64)(input)
    mu = tf.keras.layers.Dense((k), activation=None, name='mean_layer',dtype = tf.float64)(layer)
    # variance (should be greater than 0 so we exponentiate it)
    var_layer = tf.keras.layers.Dense((k), activation=None, name='dense_var_layer')(layer)
    var = tf.keras.layers.Lambda(lambda x: tf.math.exp(x), output_shape=(k,), name='variance_layer',dtype = tf.float64)(var_layer)
    # mixing coefficient should sum to 1.0
    pi = tf.keras.layers.Dense(k, activation='softmax', name='pi_layer',dtype = tf.float64)(layer)

    
    losses = []
    EPOCHS = 10000
    print_every = int(EPOCHS/100)
    
    # Define model and optimizer
    
    class gdnn(tf.keras.Model):
        def __init__(self,input_dims,output_dimensions, num_mixtures):
            super(gdnn,self).__init__()
            self.in_dim = input_dims
            self.out_dim = output_dimensions
            self.k = num_mixtures
            #create a network of:
            '''
                Input layer
                  |-Mu
                  |
                ->|-Sigma
                  |
                  |-Pi
            '''
            self.mu = tf.keras.layers.Dense(self.k*self.out_dim)
            self.sigma = tf.keras.layers.Dense(self.k*self.out_dim,activation = elu_plus)
            self.pi = tf.keras.layers.Dense(self.k)
            self.built = False
        def build(self,input_shape):
            self.mu.build(input_shape)
            self.sigma.build(input_shape)
            self.pi.build(input_shape)
            self.built = True
        
        def call(self,x):
            xs = x.shape
            assert xs[1] == self.in_dim,"Input to model not of correct size"
            if not self.built:
                self.build(xs)
            return tf.concat([self.mu(x),self.sigma(x), self.pi(x)])

        
            
            
    
    
    model = tf.keras.models.Model(input, [pi, mu, var])
    optimizer = tf.keras.optimizers.Adam(1e-2)
    model.summary()
    #model.compile(optimizer, mdn_loss)
    N = np.asarray(X_full).shape[0]
    
    dataset = tf.data.Dataset \
    .from_tensor_slices((X_full, sample_profiles_logged.reshape((N,1)))) \
    .shuffle(N).batch(N)
    
    # Start training
    print('Print every {} epochs'.format(print_every))
    best_model = model
    best_loss = np.inf
    min_diff = 100  #differential loss
    i = 0
    training_bool = i in range(EPOCHS)
    counter = 0
    counter_max = 100
    while training_bool:
        for train_x, train_y in dataset:
            loss = train_step(model, optimizer, train_x, train_y)
            losses.append(loss)
            if loss > best_loss:
                counter += 1
            
            if len(losses) > 1:
                diff = losses[-1] - losses[-2]
                if diff < min_diff:
                    min_diff = diff
                    counter = 0 #keep going if differential low enough, even if loss > min
            if loss < best_loss:
                print("Epoch {}/{}: new best loss: {}".format(i,EPOCHS,losses[-1]))
                best_loss = loss
                best_model = tf.keras.models.clone_model(model)
                counter = 0
        if i % print_every == 0:
            print('Epoch {}/{}: loss {}, Epochs since best loss: {}'.format(i, EPOCHS, losses[-1],counter))       
        i = i+1
        training_bool = (i in range(EPOCHS)) and (counter < counter_max)
    print("Training completed after {}/{} epochs. Counter: {}:: Best Loss: {}".format(i, EPOCHS, counter, best_loss))
    
    plt.figure()
    plt.plot(losses)
    plt.title("MDN Loss")
    plt.xlabel("Epoch")
    plt.ylabel("NLL Loss")
    
    
    test_profiles,t_profile_params,t_associated_r = EinastoSim.generate_n_random_einasto_profile_maggie(100)
    t_sample_profiles_logged = np.asarray([np.log(p) for p in test_profiles]).astype(np.float64)
    X_test = create_input_vectors(t_profile_params,t_associated_r)
    
    pi_test, mu_test,var_test = best_model.predict(np.asarray(X_test))
    sample_preds = sample_predictions(pi_test,mu_test,var_test)
    
    first_profile_sample = sample_preds[:100,:10,0]
    first_test_prof = t_sample_profiles_logged[0]
    plt.figure()
    plt.plot(t_associated_r[0],first_test_prof,label = "True profile")
    for j in range(10):
        plt.plot(t_associated_r[0],first_profile_sample[:,j], label = "Sample {}".format(j))
    plt.legend()
    plt.title(EinastoSim.print_params_maggie(t_profile_params[0]).replace("\t",""))
    plt.xlabel("Radius [Mpc]")
    plt.ylabel("log({}) []".format(u"\u03C1"))