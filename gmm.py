# -*- coding: utf-8 -*-
"""
Created on Sun May 24 10:07:00 2020

@author: modified from https://www.katnoria.com/mdn/ , a tutorial on tf2 gdns

Made to fit the Einasto profile data generated by Martin Sanner

"""

import numpy as np
import matplotlib.pyplot as plt
import EinastoSim
import h5py




import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
import tensorflow as tf
from copy import deepcopy

def calc_pdf(y, mu, var):
    """Calculate component density"""
    value = tf.subtract(y, mu)**2
    value = (1/tf.math.sqrt(2 * np.pi * var)) * tf.math.exp((-1/(2*var)) * value)
    return value

def pdf_np(y, mu, var):
    n = np.exp((-(y-mu)**2)/(2*var))
    d = np.sqrt(2 * np.pi * var)
    return n/d

def mdn_loss(y_true, pi, mu, var):
    """MDN Loss Function
    The eager mode in tensorflow 2.0 makes is extremely easy to write 
    functions like these. It feels a lot more pythonic to me.
    """
    out = calc_pdf(y_true, mu, var)
    # multiply with each pi and sum it
    out = tf.multiply(out, pi)
    out = tf.reduce_sum(out, 1, keepdims=True)
    out = -tf.math.log(out + 1e-10)
    return tf.reduce_mean(out)


@tf.function
def train_step(model, optimizer, train_x, train_y):
    # GradientTape: Trace operations to compute gradients
    with tf.GradientTape() as tape:
        pi_, mu_, var_ = model(train_x, training=True)
        # calculate loss
        loss = mdn_loss(train_y, pi_, mu_, var_)
    # compute and apply gradients
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss

def sample_predictions(pi_vals, mu_vals, var_vals, samples=10):
    n, k = pi_vals.shape
    # print('shape: ', n, k, l)
    # place holder to store the y value for each sample of each row
    out = np.zeros((n, samples, l))
    for i in range(n):
        for j in range(samples):
            # for each sample, use pi/probs to sample the index
            # that will be used to pick up the mu and var values
            idx = np.random.choice(range(k), p=pi_vals[i])
            for li in range(l):
                # Draw random sample from gaussian distribution
                out[i,j,li] = np.random.normal(mu_vals[i, idx*(li+l)], np.sqrt(var_vals[i, idx]))
    return out    

if __name__ == "__main__":
    sample_profiles,profile_params,associated_r = EinastoSim.generate_n_profiles(1000)
    sample_profiles_logged = np.asarray([np.log(p) for p in sample_profiles])
    EinastoSim.print_params(profile_params[0])
    
    def create_input_vectors(profile_params, assoc_r):
        assert len(assoc_r) == len(profile_params), "mismatch between parameter and r lengths"
        N = len(assoc_r)
        print("Generating {} elements from {} pairs".format(len(assoc_r[0])*N,N))
        input_vecs = []
        for i in range(N):
            base_vec = profile_params[i].copy()
            current_vec = []
            for j in range(len(assoc_r[i])):
                current_vec = base_vec.copy()
                r = assoc_r[i][j]
                current_vec.append(r)
                assert len(current_vec) == (len(base_vec) +1), "Appended more than one element"
                input_vecs.append(current_vec)
        return input_vecs
    
    X_full = create_input_vectors(profile_params, associated_r) 
    X_full = np.asarray(X_full).astype(np.float64)
    l = len(profile_params[0])+1    #current r and all params
    # Number of gaussians to represent the multimodal distribution
    k = 4
    # Network
    input = tf.keras.Input(shape=(l,))
    layer = tf.keras.layers.Dense(50, activation='tanh', name='baselayer',dtype = tf.float64)(input)
    mu = tf.keras.layers.Dense((l * k), activation=None, name='mean_layer')(layer)
    # variance (should be greater than 0 so we exponentiate it)
    var_layer = tf.keras.layers.Dense((k), activation=None, name='dense_var_layer')(layer)
    var = tf.keras.layers.Lambda(lambda x: tf.math.exp(x), output_shape=(k,), name='variance_layer')(var_layer)
    # mixing coefficient should sum to 1.0
    pi = tf.keras.layers.Dense(k, activation='softmax', name='pi_layer')(layer)
    
    model = tf.keras.models.Model(input, [pi, mu, var])
    optimizer = tf.keras.optimizers.Adam()
    model.summary()
    
    losses = []
    EPOCHS = 10000
    print_every = int(0.1 * EPOCHS)
    
    # Define model and optimizer
    model = tf.keras.models.Model(input, [pi, mu, var])
    optimizer = tf.keras.optimizers.Adam()
    N = np.asarray(X_full).shape[0]
    
    dataset = tf.data.Dataset \
    .from_tensor_slices((X_full, sample_profiles_logged.reshape((N,1)))) \
    .shuffle(N).batch(N)
    
    # Start training
    print('Print every {} epochs'.format(print_every))
    for i in range(EPOCHS):
        for train_x, train_y in dataset:
            loss = train_step(model, optimizer, train_x, train_y)
            losses.append(loss)
        if i % print_every == 0:
            print('Epoch {}/{}: loss {}'.format(i, EPOCHS, losses[-1]))       
