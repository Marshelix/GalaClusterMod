# -*- coding: utf-8 -*-
"""
Created on Sun May 24 10:07:00 2020

@author: modified from https://www.katnoria.com/mdn/ , a tutorial on tf2 gdns

Made to fit the Einasto profile data generated by Martin Sanner

"""

import numpy as np
import matplotlib.pyplot as plt
import EinastoSim
import h5py




import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
import tensorflow as tf
#import tensorflow_addons as tfa #AdamW
from copy import deepcopy
import sys
import  logging

'''
Logging: Taken from https://stackoverflow.com/a/13733863
'''
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("logfile.log"),
        logging.StreamHandler(sys.stdout)
    ]
)

def calc_pdf(y, mu, var):
    """Calculate component density"""
    value = tf.subtract(y, mu)**2
    value = (1/tf.math.sqrt(2 * np.pi * var)) * tf.math.exp((-1/(2*var)) * value)
    return value

def pdf_np(y, mu, var):
    n = np.exp((-(y-mu)**2)/(2*var))
    d = np.sqrt(2 * np.pi * var)
    return n/d

def mdn_loss(y_true, pi, mu, var):
    """MDN Loss Function
    The eager mode in tensorflow 2.0 makes is extremely easy to write 
    functions like these. It feels a lot more pythonic to me.
    """
    #throw away first few y values
    out = calc_pdf(y_true, mu, var)
    # multiply with each pi and sum it
    out = tf.multiply(out, pi)
    out = tf.reduce_sum(out, 1, keepdims=True)
    out = -tf.math.log(out + 1e-10)
   # logging.debug(tf.reduce_mean(out))
    return tf.reduce_mean(out)


@tf.function
def train_step(model, optimizer, train_x, train_y):
    # GradientTape: Trace operations to compute gradients
    with tf.GradientTape() as tape:
        pi_, mu_, var_ = model(train_x, training=True)
        # calculate loss
        loss = mdn_loss(train_y, pi_, mu_, var_)
    # compute and apply gradients
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss

def sample_predictions(pi_vals, mu_vals, var_vals, samples=10):
    n, k = pi_vals.shape
    l_out = 1
    # place holder to store the y value for each sample of each row
    out = np.zeros((n, samples, l_out))
    for i in range(n):
        for j in range(samples):
            # for each sample, use pi/probs to sample the index
            # that will be used to pick up the mu and var values
            idx = np.random.choice(range(k), p=pi_vals[i])
            for li in range(l_out):
                #for kdist in range(k):
                #    out[i,j,li] += pi_vals[i][kdist]*np.random.normal(mu_vals[i,kdist*(li+l_out)],np.sqrt(var_vals[i,kdist]))
                # Draw random sample from gaussian distribution
                out[i,j,li] = np.random.normal(mu_vals[i, idx*(li+l_out)], np.sqrt(var_vals[i, idx]))
    return out    

def mse_error(y_true, y_sample):
    '''
    Generates an average MSE fit between a true distribution and a sequence of samples
    '''
    if np.asarray(y_true.shape).size > 1:
        n,l = y_true.shape
    else:
        n = len(y_true)
    n2, num_samples = y_sample.shape
    assert n2 == n, "Sample has different granularity from original sequence"
    error = np.sum([np.dot(np.transpose(y_true-y_sample[:,k]),y_true-y_sample[:,k]) for k in range(num_samples)])
    error /= num_samples
    return error

#fixed sigma activation
# taken from https://github.com/cpmpercussion/keras-mdn-layer/blob/master/mdn/__init__.py
def elu_plus(x):
    return tf.keras.activations.elu(x)+1

if __name__ == "__main__":
    sample_profiles,profile_params,associated_r = EinastoSim.generate_n_random_einasto_profile_maggie(1000)
    sample_profiles_logged = np.asarray([np.log(p) for p in sample_profiles]).astype(np.float64)
    #EinastoSim.print_params(profile_params[0])
    
    def create_input_vectors(profile_params, assoc_r):
        assert len(assoc_r) == len(profile_params), "mismatch between parameter and r lengths"
        N = len(assoc_r)
        logging.info("Generating {} elements from {} pairs".format(len(assoc_r[0])*N,N))
        input_vecs = []
        for i in range(N):
            base_vec = profile_params[i].copy()
            current_vec = []
            for j in range(len(assoc_r[i])):
                current_vec = base_vec.copy()
                r = assoc_r[i][j]
                current_vec.append(r)
                assert len(current_vec) == (len(base_vec) +1), "Appended more than one element"
                input_vecs.append(current_vec)
        return input_vecs
    
    X_full = create_input_vectors(profile_params, associated_r) 
    X_full = np.asarray(X_full).astype(np.float64)
    l = 1+len(profile_params[0])    #current r and all params
    # Number of gaussians to represent the multimodal distribution
    k = 4
    # Network
    input = tf.keras.Input(shape=(l,))
    input_transfer_layer = tf.keras.layers.Dense(1,activation = None,dtype = tf.float64)
    layer = tf.keras.layers.Dense(50, activation='tanh', name='baselayer',dtype = tf.float64)(input)
    mu = tf.keras.layers.Dense((k), activation=None, name='mean_layer',dtype = tf.float64)(layer)
    # variance (should be greater than 0 so we exponentiate it)
    var_layer = tf.keras.layers.Dense((k), activation=None, name='dense_var_layer')(layer)
    var = tf.keras.layers.Lambda(lambda x: tf.math.exp(x), output_shape=(k,), name='variance_layer',dtype = tf.float64)(var_layer)
    # mixing coefficient should sum to 1.0
    pi = tf.keras.layers.Dense(k, activation='softmax', name='pi_layer',dtype = tf.float64)(layer)

    
    losses = []
    EPOCHS = 10000
    print_every = int(EPOCHS/100)
    
    # Define model and optimizer
    
    class gdnn(tf.keras.Model):
        def __init__(self,input_dims,output_dimensions, num_mixtures):
            super(gdnn,self).__init__()
            self.in_dim = input_dims
            self.out_dim = output_dimensions
            self.k = num_mixtures
            #create a network of:
            '''
                Input layer
                  |-Mu
                  |
                ->|-Sigma
                  |
                  |-Pi
            '''
            self.mu = tf.keras.layers.Dense(self.k*self.out_dim)
            self.sigma = tf.keras.layers.Dense(self.k*self.out_dim,activation = elu_plus)
            self.pi = tf.keras.layers.Dense(self.k)
            self.built = False
        def build(self,input_shape):
            self.mu.build(input_shape)
            self.sigma.build(input_shape)
            self.pi.build(input_shape)
            self.built = True
        
        def call(self,x):
            xs = x.shape
            assert xs[1] == self.in_dim,"Input to model not of correct size"
            if not self.built:
                self.build(xs)
            return tf.concat([self.mu(x),self.sigma(x), self.pi(x)])

        
            
            
    
    
    model = tf.keras.models.Model(input, [pi, mu, var])
    optimizer = tf.keras.optimizers.SGD(1e-3,1e-2)#tf.keras.optimizers.Adam(1e-3)
    model.summary()
    #model.compile(optimizer, mdn_loss)
    N = np.asarray(X_full).shape[0]
    
    dataset = tf.data.Dataset \
    .from_tensor_slices((X_full, sample_profiles_logged.reshape((N,1)))) \
    .shuffle(N).batch(N)
    
    # Start training
    logging.debug('Print every {} epochs'.format(print_every))
    best_model = model
    best_loss = np.inf
    max_diff = 0.0  #differential loss
    i = 0
    training_bool = i in range(EPOCHS)
    counter = 0
    counter_max = 100
    counters = []
    
    MSEs = []
    train_testing_profile, tt_p_para,t_a_r = EinastoSim.generate_n_random_einasto_profile_maggie(1)
    ttp_logged = np.asarray([np.log(p) for p in train_testing_profile]).astype(np.float64)
    X_tt = create_input_vectors(tt_p_para,t_a_r)
    
    overlap_ratios = []
    num_samples = 10
    likelihood_minimum = 0.9
    loss_maximum = -np.log(likelihood_minimum)
    diff = 0
    while training_bool:
        for train_x, train_y in dataset:
            loss = train_step(model, optimizer, train_x, train_y)
            losses.append(loss)
            likelihood = np.exp(-loss.numpy())
            if loss > best_loss:
                counter += 1
            
            if len(losses) > 1:
                diff = losses[-1] - losses[-2]
                if diff < max_diff:
                    counter += 1
                elif diff > max_diff:
                    max_diff = diff
                    counter -= 1 #keep going if differential low enough, even if loss > min
                    counter = max([0,counter]) #keep > 0
            if loss < best_loss:
                logging.info("Epoch {}/{}: new best loss: {}; Likelihood: {} | Counter: {}".format(i,EPOCHS,losses[-1],likelihood, counter))
                best_loss = loss
                best_model = tf.keras.models.clone_model(model)
                counter = 0
        #calculate mse
        pi_tt,mu_tt,var_tt = best_model.predict(np.asarray(X_tt))
        sample_preds = sample_predictions(pi_tt,mu_tt,var_tt,num_samples)
        profile_sample = sample_preds[:,:,0]
        mse_error_profiles = mse_error(ttp_logged[0],profile_sample)
        MSEs.append(mse_error_profiles)
        
        #calculate overlap
        source_overlap = np.dot(np.transpose(ttp_logged[0]),ttp_logged[0]) #ignore constant multiplier
        generation_overlaps = [np.dot(np.transpose(profile_sample[:,k]),profile_sample[:,k]) for k in range(num_samples)]
        overlap_ratio = source_overlap/np.mean(generation_overlaps)
        overlap_ratios.append(overlap_ratio)
        
        
        counters.append(counter)
        
        training_bool = i in range(EPOCHS)
        
        loss_break = (best_loss.numpy() < loss_maximum) and (np.exp(-best_loss.numpy()) > likelihood_minimum) #equivalent frankly, just redundant
        loss_break = loss_break or (diff < 0) 
        training_bool = training_bool or (not loss_break and (counter < counter_max))
        if i % print_every == 0:
            logging.info('Epoch {}/{}: loss {}, Epochs since best loss: {}; Likelihood: {}; MSE: {}; overlap: {}'.format(i, EPOCHS, losses[-1],counter,likelihood,mse_error_profiles, overlap_ratio))       
        i = i+1
        
    logging.info("Training completed after {}/{} epochs. Counter: {}:: Best Loss: {}".format(i, EPOCHS, counter, best_loss))
    
    plt.figure()
    plt.plot(losses)
    plt.title("MDN Loss")
    plt.xlabel("Epoch")
    plt.ylabel("NLL Loss")
    
    
    plt.figure()
    plt.plot(counters)
    plt.title("Counter values")
    plt.xlabel("Epoch")
    plt.ylabel("Counter")
    
    plt.figure()
    plt.plot(MSEs)
    plt.title("MSE")
    plt.xlabel("Epoch")
    plt.ylabel("Pseudo MSE")
    
    plt.figure()
    plt.plot(overlap_ratios)
    plt.title("Profile Overlap Ratios true/generated")
    plt.xlabel("Epoch")
    plt.ylabel("Overlap")
    
    test_profiles,t_profile_params,t_associated_r = EinastoSim.generate_n_random_einasto_profile_maggie(1)
    t_sample_profiles_logged = np.asarray([np.log(p) for p in test_profiles]).astype(np.float64)
    X_test = create_input_vectors(t_profile_params,t_associated_r)
    
    pi_test, mu_test,var_test = best_model.predict(np.asarray(X_test))
    sample_preds = sample_predictions(pi_test,mu_test,var_test)
    
    first_profile_sample = sample_preds[:100,:10,0]
    first_test_prof = t_sample_profiles_logged[0]
    plt.figure()
    plt.plot(t_associated_r[0],first_test_prof,label = "True profile")
    for j in range(10):
        plt.plot(t_associated_r[0],first_profile_sample[:,j], label = "Sample {}".format(j))
    plt.legend()
    plt.title(EinastoSim.print_params_maggie(t_profile_params[0]).replace("\t",""))
    plt.xlabel("Radius [Mpc]")
    plt.ylabel("log({}) []".format(u"\u03C1"))